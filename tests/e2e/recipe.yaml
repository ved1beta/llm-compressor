quant_stage:
  quant_modifiers:
    SmoothQuantModifier:
      smoothing_strength: 0.8
      mappings:
      - - ['re:.*q_proj', 're:.*k_proj', 're:.*v_proj']
        - re:.*input_layernorm
      - - ['re:.*gate_proj', 're:.*up_proj']
        - re:.*post_attention_layernorm
    GPTQModifier:
      sequential_update: false
      ignore: [lm_head]
      config_groups:
        group_0:
          weights: {num_bits: 8, type: int, symmetric: true, strategy: channel}
          input_activations: {num_bits: 8, symmetric: false}
          targets: [Linear]
        # Dynamic asymmetric W8A8-Int8 configuration
        group_1:
          weights: {num_bits: 8, type: int, symmetric: true, strategy: channel}
          input_activations: {num_bits: 8, symmetric: false, dynamic: true}
          targets: [Linear]
        # Static asymmetric W8A8-Int8 configuration
        group_2:
          weights: {num_bits: 8, type: int, symmetric: true, strategy: channel}
          input_activations: {num_bits: 8, symmetric: false, dynamic: false}
          targets: [Linear]
